{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### I. The \"Framingham Heart Study\" Data Set\n",
                "\n",
                "In this exercise we use the Framingham data set to predict whether a respondent is above a certain age.\n",
                "\n",
                "Read in the data set, call it \"`full`\", and drop observations with at least one missing value\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "full = read.csv(\"HSML 6295 ds Framingham.csv\")\n",
                "full = na.omit(full)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Show the structure of the data set\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "str(full)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Show summary statistics \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(stargazer)\n",
                "stargazer(full, \n",
                "          type = \"text\", \n",
                "          summary.stat = c(\"n\", \"mean\", \"sd\", \"min\", \"p25\", \"median\", \"p75\", \"max\"),\n",
                "          title=\"Full Data Set\", digits=1)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Define the the `response` variable as: \"The respondent was at least 48 years old at the time of the survey.\"\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "full$response = ifelse(full$Age >= 48, 1, 0)\n",
                "table(full$response)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "To build most of the predictive models in this software practice and to create the confusion matrices, it is useful to declare the response variable to be a factor variable with two levels, labeled \"Yes\" and \"No\"\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "full$response = factor(full$response, levels = c(0,1), labels = c(\"No\", \"Yes\"))\n",
                "table(full$response)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Drop `Age` from the data set and move the new `response` variable from the last (16th) to first position in data set \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "full = subset(full, select = -c(Age))\n",
                "full = subset(full, select = c(16, 1:15))\n",
                "# show list of variables in current data set\n",
                "names(full)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Create a list called `train_id` of 3,658/2 = 1,829 random numbers between 1 and 3,658, the number of observations in the \"`full`\" data set.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "set.seed (12345)\n",
                "train_id = sample(1:nrow(full), nrow(full)/2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Split the full data set into two subsets of equal sample size, called \"`train`\" and \"`test`\".\n",
                "To do so, use the random numbers in the `train_id` list created above to tag the observations that will be assigned to the training set. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train = full[train_id,]\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Assign the observations whose ID number is not included in the `train_id` list to the test set\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test = full[-train_id,]\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Compute summary statistics for the training set\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "stargazer(train, \n",
                "          type = \"text\", \n",
                "          summary.stat = c(\"n\", \"mean\", \"sd\", \"min\", \"p25\", \"median\", \"p75\", \"max\"),\n",
                "          title=\"Training Set\", digits=1)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Compute summary statistics for the test set\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "stargazer(test, \n",
                "          type = \"text\", \n",
                "          summary.stat = c(\"n\", \"mean\", \"sd\", \"min\", \"p25\", \"median\", \"p75\", \"max\"),\n",
                "          title=\"Test Set\", digits=1)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Note that while the maximum values of most continuous variables vary widely between the training and test sets, the differences in mean and median values are never larger than one unit and sometimes zero.\n",
                "\n",
                "Define the accuracy, true positive rate (TPR), and false positive rate (FPR) achieved by a given predictive model as functions of the observed (`Actual`) and predicted (`Predicted`) responses.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "accuracy = function(Actual, Predicted) {\n",
                "    round(100*mean(Actual == Predicted),2)\n",
                "}\n",
                "# true positive rate\n",
                "TPR = function(Actual, Predicted) {\n",
                "    round(100*sum((Actual==\"Yes\")*(Predicted==\"Yes\"))/sum(Actual==\"Yes\"),2)\n",
                "}\n",
                "# false positive rate\n",
                "FPR = function(Actual, Predicted) {\n",
                "    round(100*sum((Actual==\"No\")*(Predicted==\"Yes\"))/sum(Actual==\"No\"),2)\n",
                "}\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Calculate the number of predictor variables\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "predictors = ncol(full)-1\n",
                "predictors\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### II. Bayes Classifier\n",
                "\n",
                "The simplest prediction assigns to each observation the modal class, i.e. the class that is found most frequently in the training set, *even if this class is found in fewer than half the observations*. This classifier is commonly known as the Bayes classifier. In this software practice, the Bayes classifier is an example of a \"null model\" in that it does not make use of any predictor variables in the data set. It thus serves as a baseline model.\n",
                "\n",
                "Compute the predicted values of the response in the *test* set.\n",
                "\n",
                "1. Find the modal class in the training set\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mode = function(x) {\n",
                "  unique_x = unique(x)\n",
                "  unique_x[which.max(tabulate(match(x, unique_x)))]\n",
                "}\n",
                "mode(train$response)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "2. Assign this class to *all* observations in the test set\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "predicted = rep(mode(train$response), length(test$response))\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Generate the confusion matrix for the test set and compute the accuracy, true positive rate (TPR), and false positive rate (FPR).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cm = table(Actual = test$response, Predicted = predicted)\n",
                "(cm = addmargins(cm, FUN = list(Total = sum), quiet = TRUE))\n",
                "(accuracy_Bayes = accuracy(Actual = test$response, Predicted = predicted))\n",
                "(TPR_Bayes = TPR(Actual = test$response, Predicted = predicted))\n",
                "(FPR_Bayes = FPR(Actual = test$response, Predicted = predicted))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### III. Logistic Regression\n",
                "\n",
                "Compute the logistic regression fit of `response` on all 15 predictor variables on the training set  and save the result as `logistic`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "logistic = glm(response ~ ., data=train, family=binomial)\n",
                "round(coef(summary(logistic)),2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Compute the predicted values of the response in the *test* set.\n",
                "\n",
                "1. Compute the predicted probability for each observation in the test set\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "predicted = predict(logistic, newdata=test, type=\"response\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "2. Convert the predicted probability into a predicted class using the probability threshold of 0.5\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "predicted = ifelse(predicted > 0.5, \"Yes\", \"No\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Generate the confusion matrix for the test set and compute the accuracy, true positive rate (TPR), and false positive rate (FPR).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cm = table(Actual = test$response, Predicted = predicted)\n",
                "(cm = addmargins(cm, FUN = list(Total = sum), quiet = TRUE))\n",
                "(accuracy_logistic = accuracy(Actual = test$response, Predicted = predicted))\n",
                "(TPR_logistic = TPR(Actual = test$response, Predicted = predicted))\n",
                "(FPR_logistic = FPR(Actual = test$response, Predicted = predicted))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### IV. Ridge Regression\n",
                "\n",
                "Declare matrix of predictors `x` and response variable `y` and define the list (\"`grid`\") of $\\lambda$ (lambda) values for which the ridge regression model is fit:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x = model.matrix(response ~ ., data=train)[,-1]\n",
                "y = train$response\n",
                "grid=10^seq(-0.8,-2.8,length=40)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Compute value of $\\lambda$, stored as `cv$lambda.min`, that minimizes the training error, defined as the cross-validated prediction error for the *training* set. To fit a ridge regression model, we set `alpha` to 0.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(glmnet)\n",
                "set.seed (1)\n",
                "cv = cv.glmnet(x, y, alpha=0, family = \"binomial\", lambda = grid)\n",
                "cv$lambda.min\n",
                "plot(cv)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The horizontal axis in this graph is drawn at logarithmic scale to show more detail. \"Log\" refers to the natural logarithm, also abbreviated as \"ln\".\n",
                "The value of $\\lambda$ shown at the left dotted line, \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "round(log(cv$lambda.min),2)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": ", is the value that minimizes the training error (`cv$lambda.min`). The red dots are the point estimates of the prediction error and the gray bars are one standard error above and below the red dots. The right dotted line in the graph marks the value of $\\lambda$ whose point estimate is one standard error larger than that of `cv$lambda.min`:\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "round(cv$lambda.1se,4)\n",
                "round(log(cv$lambda.1se),2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "You can think of this \"second-best\" value of $\\lambda$ as the largest value of $\\lambda$ that is statistically indistinguishable from `cv$lambda.min`. If our goal is to shrink the coefficient estimates as much as possible, we could choose a value of $\\lambda$ as high as \n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "round(cv$lambda.1se, 4)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Compute the coefficient estimates for the ridge regression model that corresponds to `cv$lambda.min` and save the result as `ridge`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ridge = glmnet(x, y, alpha=0, lambda=cv$lambda.min, family = \"binomial\")\n",
                "round(coef(ridge),2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The numbers of predictors included in the various ridge regression fits are shown above the top horizontal axis in the graph above. When we fit ridge regression models, the coefficient estimates are \"shrunk\", i.e. their absolute magnitude is reduced. For instance, the coefficient estimate for the predictor `Stroke` is 1.16 in the logistic regression fit but 1.12 in the ridge regression fit. Similarly, the coefficient estimate for the predictor `Smoker` has shrunk from -0.59 in the logistic regression fit to -0.57 in the ridge regression fit.\n",
                "\n",
                "If we wanted to shrink the coefficient estimates even further, we could use the \"second-best\" value of $\\lambda$:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ridge.2 = glmnet(x, y, alpha=0, lambda=cv$lambda.1se, family = \"binomial\")\n",
                "round(coef(ridge.2),2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The larger value of $\\lambda$ has shrunk the coefficients for `Stroke` and `Smoker` even further, to 0.75 and -0.42, respectively. \n",
                "\n",
                "Compute the predicted values of the response in the *test* set.\n",
                "\n",
                "1. Compute the predicted probability for each observation in the test set\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x = model.matrix(response ~ ., test)[,-1]\n",
                "predicted = predict(ridge, s=cv$lambda.min, newx=x, type = \"response\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "2. Convert the predicted probability into a predicted class using the probability threshold of 0.5\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "predicted = ifelse(predicted > 0.5, \"Yes\", \"No\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Generate the confusion matrix for the test set and compute the accuracy, true positive rate (TPR), and false positive rate (FPR).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cm = table(Actual = test$response, Predicted = predicted)\n",
                "(cm = addmargins(cm, FUN = list(Total = sum), quiet = TRUE))\n",
                "(accuracy_ridge = accuracy(Actual = test$response, Predicted = predicted))\n",
                "(TPR_ridge = TPR(Actual = test$response, Predicted = predicted))\n",
                "(FPR_ridge = FPR(Actual = test$response, Predicted = predicted))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### V. The Lasso\n",
                "\n",
                "Compute value of $\\lambda$, stored as `cv$lambda.min`, that minimizes the training error, defined as the cross-validated prediction error for the *training* set.\n",
                "To fit a lasso model, we set `alpha` to 1.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x = model.matrix(response ~ ., data=train)[,-1]\n",
                "y = train$response\n",
                "grid=10^seq(-1.8,-2.6,length=40)\n",
                "\n",
                "library(glmnet)\n",
                "set.seed (1)\n",
                "cv = cv.glmnet(x, y, alpha=1, family = \"binomial\", lambda = grid)\n",
                "cv$lambda.min\n",
                "plot(cv)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "In the graph, the value of $\\lambda$ that minimizes the cross-validated training error, `cv$lambda.min`, is shown at \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "round(log(cv$lambda.min),2)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                ". The top horizontal axis in the graph shows that the lasso model that minimizes the training error includes only 12 predictors.\n",
                "\n",
                "To see which predictors the optimal lasso model has dropped, we compute the coefficient estimates for the lasso model that corresponds to `cv$lambda.min` and save the result as `lasso`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "lasso = glmnet(x, y, alpha=1, lambda=cv$lambda.min, family = \"binomial\")\n",
                "round(coef(lasso),2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The optimal lasso model no longer includes the predictors `BP.Medication`, `BMI`, and `Glucose`.\n",
                "Also, note that the absolute magnitudes of the coefficients of the other predictors have shrunk. For instance, the coefficients for `Stroke` and `Smoker` are now 0.80 and -0.57, respectively.\n",
                "\n",
                "If we wanted to drop even more predictors (and shrink the remaining non-zero coefficient estimates even further), we could use the \"second-best\" value of $\\lambda$, which is shown at the right dotted line in the graph:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "round(cv$lambda.1se, 4)\n",
                "round(log(cv$lambda.1se),2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The top horizontal axis of the graph shows that this more restrictive model only includes 10 predictors. \n",
                "The corresponding lasso model is:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "lasso.2 = glmnet(x, y, alpha=1, lambda=cv$lambda.1se, family = \"binomial\")\n",
                "round(coef(lasso.2),2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In addition to `BP.Medication`, `BMI`, and `Glucose`, the more restrictive lasso model drops (shrinks to zero) the coefficients for the predictors `Stroke` and `Male`. Also, the coefficient for `Smoker` has shrunk from -0.57 to -0.51.\n",
                "\n",
                "Compute the predicted values of the response in the *test* set.\n",
                "\n",
                "1. Compute the predicted probability for each observation in the test set\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x = model.matrix(response ~ ., test)[,-1]\n",
                "predicted = predict(lasso, s=cv$lambda.min, newx=x, type = \"response\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "2. Convert the predicted probability into a predicted class using the probability threshold of 0.5\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "predicted = ifelse(predicted > 0.5, \"Yes\", \"No\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Generate the confusion matrix for the test set and compute the accuracy, true positive rate (TPR), and false positive rate (FPR).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cm = table(Actual = test$response, Predicted = predicted)\n",
                "(cm = addmargins(cm, FUN = list(Total = sum), quiet = TRUE))\n",
                "(accuracy_lasso = accuracy(Actual = test$response, Predicted = predicted))\n",
                "(TPR_lasso = TPR(Actual = test$response, Predicted = predicted))\n",
                "(FPR_lasso = FPR(Actual = test$response, Predicted = predicted))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Note that we could have obtained the logistic regression model by estimating a ridge (`alpha` = 0) or lasso (`alpha` = 1) regression model and setting `lambda` = 0. (Minimal differences between the coefficient values are due to rounding.)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(glmnet)\n",
                "x = model.matrix(response ~ ., data=train)[,-1]\n",
                "y = train$response\n",
                "logistic.ridge = glmnet(x, y, alpha=0, lambda=0, family = \"binomial\")\n",
                "logistic.lasso = glmnet(x, y, alpha=1, lambda=0, family = \"binomial\")\n",
                "round(coef(summary(logistic)),2)\n",
                "round(coef(logistic.ridge),2)\n",
                "round(coef(logistic.lasso),2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### VI. Single Pruned Tree\n",
                "\n",
                "Using the training set, grow the unpruned (\"fully grown\") tree and save the result as `tree`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(tree)\n",
                "tree = tree(response ~ ., data=train)\n",
                "summary(tree)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Generate the confusion matrix for the *training* set.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "predicted = predict(tree, newdata=train, type = \"class\")\n",
                "cm = table(Actual = train$response, Predicted = predicted)\n",
                "cm = addmargins(cm, FUN = list(Total = sum), quiet = TRUE)\n",
                "cm\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Note that the number of misclassified patients is 601 and matches the number reported by the `summary(tree)` command above. 160 patients were false negative: the predicted condition was \"No\" when their actual condition was \"Yes\". 441 patients were false positives: the predicted condition was \"Yes\" when their actual condition was \"No\".\n",
                "\n",
                "Plot the unpruned tree\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot(tree)\n",
                "text(tree, pretty = 0)\n",
                "title(main = \"Unpruned Classification Tree \\n\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Note that the number of terminal nodes is 7, matching the number reported by the `summary(tree)` command above. Four terminal nodes predict that the patient is at least 48 years old.\n",
                "\n",
                "Compute the 20-fold cross-validated prediction error for subtrees of various sizes. The cross-validated prediction error is the average number of misclassified patients in the test set defined by each of the 20 cross-validation folds. The tree sizes are measured by the number of terminal nodes.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "set.seed(6295)\n",
                "cv = cv.tree(tree, FUN=prune.misclass, K=20)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Plot the cross-validated prediction error as a function of the tree size.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot(cv$dev ~ cv$size, type='b', col=\"lightseagreen\", lwd=2,\n",
                "     xlab = \"Subtree Size (Terminal Nodes)\", ylab = \"Cross-Validated Prediction Error\")\n",
                "axis(1, at=cv$size)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Save the size of the subtree that minimizes the cross-validated prediction error.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "arg_min_cv = cv$size[which.min(cv$dev)]\n",
                "arg_min_cv\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Prune the original tree to the size that minimizes the CV error and save the result as `pruned_tree`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pruned_tree = prune.misclass(tree, best = arg_min_cv)\n",
                "summary(pruned_tree)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Plot the pruned tree\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot(pruned_tree)\n",
                "text(pruned_tree, pretty=0)\n",
                "title(main = \"Pruned Classification Tree \\n\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To obtain the pruned tree in this example, we've simply cut back the unpruned tree to the internal nodes that matter, i.e. where going left versus right changes the prediction. For this reason, the pruned tree in this example yields the same predictions as the unpruned tree, and the resulting misclassification error rate (training error) is the same: 0.3286 = 601 / 1829.\n",
                "\n",
                "Compute the predicted values of the response in the *test* set.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "predicted = predict(pruned_tree, newdata=test, type = \"class\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Generate the confusion matrix for the test set and compute the accuracy, true positive rate (TPR), and false positive rate (FPR).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cm = table(Actual = test$response, Predicted = predicted)\n",
                "(cm = addmargins(cm, FUN = list(Total = sum), quiet = TRUE))\n",
                "(accuracy_tree = accuracy(Actual = test$response, Predicted = predicted))\n",
                "(TPR_tree = TPR(Actual = test$response, Predicted = predicted))\n",
                "(FPR_tree = FPR(Actual = test$response, Predicted = predicted))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The number of misclassified patients when we apply the pruned classification tree to the test set is 601, the same number that we found when we applied the unpruned classification tree to the training set. This is a coincidence.\n",
                "\n",
                "#### VII. Bootstrap Aggregation (Bagging)\n",
                "\n",
                "Using the training set, grow one unpruned tree for each of 500 bootstrap samples and save the result as `bag`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(randomForest)\n",
                "set.seed(1)\n",
                "bag = randomForest(response ~ ., data = train, mtry = predictors, importance = TRUE)\n",
                "bag\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Compute the predicted values of the response in the *test* set.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "predicted = predict(bag, newdata=test, type = \"class\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Generate the confusion matrix for the test set and compute the accuracy, true positive rate (TPR), and false positive rate (FPR).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cm = table(Actual = test$response, Predicted = predicted)\n",
                "(cm = addmargins(cm, FUN = list(Total = sum), quiet = TRUE))\n",
                "(accuracy_bag = accuracy(Actual = test$response, Predicted = predicted))\n",
                "(TPR_bag = TPR(Actual = test$response, Predicted = predicted))\n",
                "(FPR_bag = FPR(Actual = test$response, Predicted = predicted))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### VIII. Random Forest\n",
                "\n",
                "Random forests are grown just like the bootstrap-aggregated forests. The difference is that to grow each tree in a random forest only a subset $m$ of all available predictors is considered. To grow each tree in a bootstrap-aggregated forest *all* available predictors are considered.\n",
                "\n",
                "Define $m = \\sqrt{p}$, the number of predictors considered at each split.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "predictors\n",
                "m = round(sqrt(predictors))\n",
                "m\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Grow a random forest of 500 trees and save the result as `rf`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(randomForest)\n",
                "set.seed(1)\n",
                "rf = randomForest(response ~ ., data = train, mtry = m, importance = TRUE)\n",
                "rf\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Compute the predicted values of the response in the *test* set\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "predicted = predict(rf, newdata=test, type = \"class\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Generate the confusion matrix for the test set and compute the accuracy, true positive rate (TPR), and false positive rate (FPR).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cm = table(Actual = test$response, Predicted = predicted)\n",
                "(cm = addmargins(cm, FUN = list(Total = sum), quiet = TRUE))\n",
                "(accuracy_rf = accuracy(Actual = test$response, Predicted = predicted))\n",
                "(TPR_rf = TPR(Actual = test$response, Predicted = predicted))\n",
                "(FPR_rf = FPR(Actual = test$response, Predicted = predicted))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Plot the importance of each predictor\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "varImpPlot(rf)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### IX. Boosting\n",
                "\n",
                "Convert the response variable back to numeric format\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train$response = factor(as.numeric(train$response))\n",
                "table(train$response)\n",
                "train$response = as.numeric(train$response)-1\n",
                "table(train$response)\n",
                "mean(train$response)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Grow a sequence of 5,000 trees using the *training* set and save the result as `boost`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(gbm)\n",
                "set.seed(1)\n",
                "boost = gbm(response ~ ., data = train, \n",
                "                   distribution = \"bernoulli\", \n",
                "                   n.trees=5000, interaction.depth=1)\n",
                "summary(boost, plotit = FALSE)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Compute the predicted values of the response in the *test* set.\n",
                "\n",
                "1. Compute the predicted probability for each observation in the test set (after converting the response variable back to numeric format)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test$response = factor(as.numeric(test$response))\n",
                "test$response = as.numeric(test$response)-1\n",
                "predicted = predict(boost, newdata=test, n.trees=5000, type = \"response\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "2. Convert the predicted probability into a predicted class using the probability threshold of 0.5\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "predicted = ifelse(predicted > 0.5, \"Yes\", \"No\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Generate the confusion matrix for the test set and compute the accuracy, true positive rate (TPR), and false positive rate (FPR).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cm = table(Actual = as.factor(ifelse(test$response == 1, \"Yes\", \"No\")), Predicted = predicted)\n",
                "(cm = addmargins(cm, FUN = list(Total = sum), quiet = TRUE))\n",
                "(accuracy_boost = accuracy(Actual = as.factor(ifelse(test$response == 1, \"Yes\", \"No\")), Predicted = predicted))\n",
                "(TPR_boost = TPR(Actual = as.factor(ifelse(test$response == 1, \"Yes\", \"No\")), Predicted = predicted))\n",
                "(FPR_boost = FPR(Actual = as.factor(ifelse(test$response == 1, \"Yes\", \"No\")), Predicted = predicted))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### X. Summary\n",
                "\n",
                "The following table shows the values of 3 performance statistics for the 8 different predictive models that were optimized on the training set and evaluated on the test set. The 8 models are listed in ascending order of the false positive rate.\n",
                "\n",
                "**Prediction Method**   | **False Positive Rate**             | **True Positive Rate**             | **Accuracy**            \n",
                "---                     |  ---:                               | ---:                               | ---:                    \n",
                "Lasso\t\t\t              | 30.14\t\t\t | 71.85\t\t\t | 70.97\n",
                "Ridge Regression\t\t\t  | 30.26\t\t\t | 71.95\t\t\t | 70.97\n",
                "Logistic Regression\t\t\t| 30.63\t\t\t | 72.24\t\t\t | 70.97\n",
                "Random Forest\t\t\t      | 33.46\t\t\t | 75.39\t\t\t | 71.46\n",
                "Bagging\t\t\t            | 33.95\t\t\t | 74.90\t\t\t | 70.97\n",
                "Boosting\t\t\t          | 37.02\t\t\t | 72.24\t\t\t | 68.12\n",
                "Single Pruned Tree\t\t\t| 54.61\t\t\t | 84.55\t\t\t | 67.14\n",
                "Bayes Classifier\t\t\t  | 100.00\t\t | 100.00\t\t\t | 55.55\n",
                "\n",
                "The table shows that, with the exception of the bagging and boosting models, raising the false positive rate (FPR) raises the true positive rate (TPR). The intuition is the same as in the construction of ROC curves for a given class of predictive model: the more readily a model predicts a positive response, the more readily that model will both capture the true positives and misclassify as positives responses that actually are negative.\n",
                "\n",
                "As in the plot of a standard ROC curve, we can plot the true positive rate on the vertical axis against the false positive rate on the horizontal axis:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "FPR = c(FPR_lasso, FPR_ridge, FPR_logistic, FPR_rf, FPR_bag, FPR_boost, FPR_tree, FPR_Bayes)\n",
                "FPR_frontier = c(0, FPR_lasso, FPR_ridge, FPR_logistic, FPR_rf, FPR_tree, FPR_Bayes)\n",
                "TPR = c(TPR_lasso, TPR_ridge, TPR_logistic, TPR_rf, TPR_bag, TPR_boost, TPR_tree, TPR_Bayes)\n",
                "TPR_frontier = c(0, TPR_lasso, TPR_ridge, TPR_logistic, TPR_rf, TPR_tree, TPR_Bayes)\n",
                "roc = data.frame(FPR, TPR)\n",
                "attr(roc, \"row.names\") = c(\"lasso\", \"ridge\", \"logistic\", \"rf\", \"bag\", \"boost\", \"tree\", \"Bayes\")\n",
                "par(pty = \"s\")\n",
                "plot(FPR, TPR, xlim=c(-10,110), ylim = c(-10,110), asp=1,\n",
                "     xlab = \"False Positive Rate (FPR)\", ylab = \"True Positive Rate (TPR)\")\n",
                "with(roc, text(TPR ~ FPR, labels = row.names(roc), pos = 1, col='red3', cex = 0.8))\n",
                "lines(FPR_frontier, TPR_frontier, type='b', lwd=2, col='red3')\n",
                "title(main = \"Predictive Performance of 8 Models \\n\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "There is a cluster of models whose false positive rate is between 30% and 40%, as shown in the figure below:\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "roc$label_color = \"red3\"\n",
                "roc$label_color[roc$FPR > 33.5] = \"black\"\n",
                "attr(roc, \"row.names\") = c(paste(\"lasso\", accuracy_lasso), paste(\"ridge\", accuracy_ridge), \n",
                "                           paste(\"logistic\", accuracy_logistic), paste(\"random forest\", accuracy_rf), \n",
                "                           paste(\"bag\", accuracy_bag), paste(\"boost\", accuracy_boost), \n",
                "                           paste(\"tree\", accuracy_tree), paste(\"Bayes\", accuracy_Bayes))\n",
                "par(pty = \"m\")\n",
                "plot(FPR, TPR, xlim=c(30,38), ylim = c(71.8,75.4),\n",
                "     xlab = \"False Positive Rate (FPR)\", ylab = \"True Positive Rate (TPR)\")\n",
                "with(roc, text(TPR ~ FPR, labels = row.names(roc), pos = 4, col=roc$label_color, cex = 0.8))\n",
                "lines(FPR_frontier, TPR_frontier, type='b', lwd=2, col='red3')\n",
                "title(main = \"Predictive Performance of 6 Models \\n\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The numbers behind the model names are the models' accuracy values. In this example, 4 models -- lasso, ridge, logistic, and bag -- all yield the same accuracy but their true and false positive rates differ. Thus, if you were to choose among these 4 models, knowing only their accuracy would render them indistinguishable; knowing their TPR and FPR will reveal the trade-offs involved in choosing one model over the other 3.\n",
                "\n",
                "The 4 models shown in red lie on the frontier: for each of these models, there is no other model to its northwest, i.e. no other model simultaneously achieves a higher TPR *and* a lower FPR. We see that the bagging and boosting models both fail this test: The random-forest model achieves a higher TPR and lower FPR than the bagging and boosting models. The logistic regression model achieves the same TPR as and a lower FPR than the boosting model. The bagging and boosting models are said to be \"strictly dominated\" because we can raise or maintain the TPR while lowering the FPR by switching to a model on the frontier.\n",
                "\n",
                "The accuracy values of the two strictly dominated models are strictly smaller than those of the models that are superior in each case: the bagging model (70.97) is dominated by the random-forest model (71.46), while the boosting model (68.12) is dominated by the random-forest model (71.46) and the logistic-regression model (70.97). Also note that the accuracy (70.97) is the same for the bagging model and the logistic-regression model but neither dominates the other: the bagging model achieves a higher TPR than the logistic-regression model but it also comes with a higher FPR. In the figure, the bagging model lies to the northeast of the logistic-regression model, not the northwest.\n",
                "\n",
                "Once you've dropped the two strictly dominated models, which of the remaining 8 models, all of which lie on the frontier, should you use? The answer depends on how much you value true positives and how costly false positives are to you. Suppose you are classifying 200 patients, and you know that 100 of these are positive and 100 are negative. You just don't know each individual patient's actual status. If you use the logistic-regression model, you will catch 72.24 of the 100 positive patients. But you'll also falsely label 30.63\tof the 100 negative patients as positives. Switching to the random-forest model will enable you to raise the number of true positives by \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "round((TPR_rf - TPR_logistic),2)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "to 75.39 patients. But this switch comes at the cost of raising the number of false positives by \n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "round((FPR_rf - FPR_logistic),2)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "to 33.46. Thus, when you switch from the logistic-regression model to the random-forest model, allowing one additional false positive patient allows you to identify \n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "round((TPR_rf - TPR_logistic)/(FPR_rf - FPR_logistic),2)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "additional true positive patients on average. You can raise the number of true positives even further by switching to the single pruned tree. But this switch yields only `\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "round((TPR_tree - TPR_rf)/(FPR_tree - FPR_rf),2)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "additional true positive patients for every additional false positive patient. \n"
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
