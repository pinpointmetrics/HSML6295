{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### HSML 6295 Session 5 (Regression Trees) Revised\n",
                "\n",
                "#### I. The `HCAHPS` Data Set\n",
                "\n",
                "We use the HCAHPS (Hospital Consumer Assessment of Healthcare Providers and Systems) data set to predict a hospital's overall rating.\n",
                "\n",
                "Read in the data set, call it \"`full`\", and drop observations with at least one missing value\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "full = read.csv(\"HSML 6295 ds HCAHPS.csv\")\n",
                "full = na.omit(full)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Variable Name** | **Description** \n",
                "---               | ---         \n",
                "`nurses`          | Communication with Nurses \n",
                "`doctors`         | Communication with Doctors \n",
                "`staff`           | Responsiveness of Hospital Staff\n",
                "`care`            | Care Transition\n",
                "`meds`            | Communication about Medicines\n",
                "`clean`           | Cleanliness and Quietness of Hospital Environment\n",
                "`info`            | Discharge Information\n",
                "`rating`          | Overall Rating of Hospital\n",
                "\n",
                "Show the structure of the data set.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "str(full)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Move the response variable `rating` from the eighth to the first position and rename to `response`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "full = subset(full, select=c(8,1:7))\n",
                "names(full)[names(full)==\"rating\"] = \"response\"\n",
                "# show list of variables in current data set\n",
                "names(full)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Compute summary statistics for the full data set. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(stargazer)\n",
                "stargazer(full, \n",
                "          type = \"text\", \n",
                "          summary.stat = c(\"n\", \"mean\", \"sd\", \"min\", \"p25\", \"median\", \"p75\", \"max\"),\n",
                "          title=\"Full Data Set\", digits=1)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Calculate the number of predictor variables\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "(predictors = ncol(full)-1)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Create a list called `train_id` of 2,792/2 = 1,396 random numbers between 1 and 2,792, the number of observations in the \"`full`\" data set.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "set.seed (12345)\n",
                "train_id = sample(1:nrow(full), nrow(full)/2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Split the full data set into two subsets of equal sample size, called \"`train`\" and \"`test`\".\n",
                "To do so, use the random numbers in the `train_id` list created above to tag the observations that will be assigned to the training set. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train = full[train_id,]\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Assign the observations whose ID number is not included in the `train_id` list to the test set\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test = full[-train_id,]\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Compute summary statistics for the training set\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "stargazer(train, \n",
                "          type = \"text\", \n",
                "          summary.stat = c(\"n\", \"mean\", \"sd\", \"min\", \"p25\", \"median\", \"p75\", \"max\"),\n",
                "          title=\"Training Set\", digits=1)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Compute summary statistics for the test set\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "stargazer(test, \n",
                "          type = \"text\", \n",
                "          summary.stat = c(\"n\", \"mean\", \"sd\", \"min\", \"p25\", \"median\", \"p75\", \"max\"),\n",
                "          title=\"Test Set\", digits=1)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Note that while the maximum values of most predictors are much larger in the test set than in the training set (e.g. 89.2 versus 69.8 for `care`), the differences in mean values are never larger than one unit and the differences in median values are never larger than half a unit.\n",
                "\n",
                "#### II. Null Model\n",
                "\n",
                "The null (intercept-only) model uses the mean value of the response in the *training* set as the prediction of all values of the response in the *test* set. It is called the \"null model\" because it does not use any predictor variables.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "predicted = rep(mean(train$response), length(test$response))\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Compute the test error, defined as the mean squared error.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "(mse_null = round(mean((predicted - test$response)^2),2))\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### III. Linear Regression\n",
                "\n",
                "Fit a linear regression model to the training set.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "linear = lm(response ~ ., train)\n",
                "round(coef(summary(linear)),2)\n",
                "(r2 = round(summary(linear)$r.squared,2))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Compute the predicted values of the response in the *test* set and the test error.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "predicted = predict(linear, newdata = test)\n",
                "(mse_linear = round(mean((predicted - test$response)^2),2))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Plot the predicted against the actual ratings.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "par(pty = \"s\")\n",
                "plot(predicted ~ test$response, xlim=c(30,110), ylim = c(30,110), asp=1,\n",
                "     ylab = \"Predicted Rating\", xlab = \"Actual Rating\")\n",
                "abline(0,1)\n",
                "title(main = \"Linear Regression Model\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### IV. Ridge Regression\n",
                "\n",
                "Define `x` and `y` as the matrix of predictors and the vector of responses `y` in the training set. Also define a list (\"`grid`\") of $\\lambda$ (lambda) values for which the ridge regression model is fit.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x = model.matrix(response ~ ., data=train)[,-1]\n",
                "y = train$response\n",
                "grid=10^seq(-1,.5,length=40)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Compute value of $\\lambda$ (lambda) that minimizes the cross-validated mean squared error in the *training* set. This value is stored as `cv$lambda.min`. To fit a ridge regression model, we set the parameter `alpha` to 0. The parameter `nfolds` sets the number of cross-validation folds.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(glmnet)\n",
                "set.seed (1)\n",
                "cv = cv.glmnet(x, y, alpha=0, lambda = grid, nfolds = 10)\n",
                "round(cv$lambda.min, 4)\n",
                "plot(cv)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The horizontal axis in this graph is drawn at logarithmic scale to show more detail. \"Log\" refers to the natural logarithm.\n",
                "The value of $\\lambda$ that minimizes the cross-validated mean squared error is shown at the left dotted line:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "round(log(cv$lambda.min),2)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The red dots are the point estimates of the prediction error and the gray bars are one standard error above and below the red dots. The right dotted line in the graph marks the \"second-best\" value of $\\lambda$ whose point estimate is one standard error larger than that of `cv$lambda.min`:\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "round(cv$lambda.1se,4)\n",
                "round(log(cv$lambda.1se),2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Compute the coefficient estimates for the ridge regression model that corresponds to `cv$lambda.min` and save the result as `ridge`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ridge = glmnet(x, y, alpha=0, lambda=cv$lambda.min)\n",
                "round(coef(ridge),2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The coefficients for `nurses` and `staff` have shrunk from 0.51 and -0.10 to 0.48 and -0.08, respectively. The coefficient for `doctors` has increased from 0.11 to 0.12, however. This is possible because ridge regression models are fit by minimizing the sum of the squared coefficient estimates,\n",
                "$$ \\lambda \\sum_{j=1}^p \\beta_j^2$$\n",
                "In this example, the shrinkage of values of the coefficients for `nurses` and `staff` (and others) more than compensates for the expansion of the coefficient for `doctors`.\n",
                "\n",
                "If we wanted to shrink the coefficient estimates even further, we could use the \"second-best\" value of $\\lambda$:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ridge.2 = glmnet(x, y, alpha=0, lambda=cv$lambda.1se)\n",
                "round(coef(ridge.2),2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The larger value of $\\lambda$ has shrunk the coefficient for `nurses` from 0.48 to 0.32. The sign of the coefficient for `staff` has flipped from negative to positive but its absolute value has shrunk: it changed from -0.08 to 0.03. The coefficient for `doctors` has increased further, from 012 to 0.16.\n",
                "\n",
                "Compute the predicted values of the response in the *test* set and the test error.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x = model.matrix(response ~ ., test)[,-1]\n",
                "predicted = predict(ridge, s=cv$lambda.min, newx=x)\n",
                "(mse_ridge = round(mean((predicted - test$response)^2),2))\n",
                "predicted.2 = predict(ridge.2, s=cv$lambda.min, newx=x)\n",
                "(mse_ridge.2 = round(mean((predicted.2 - test$response)^2),2))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### V. The Lasso\n",
                "\n",
                "Define `x` and `y` as the matrix of predictors and the vector of responses `y` in the training set. Also define a list (\"`grid`\") of $\\lambda$ (lambda) values for which the lasso regression model is fit.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x = model.matrix(response ~ ., data=train)[,-1]\n",
                "y = train$response\n",
                "grid=10^seq(-1.5,-0.2,length=40)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Compute value of $\\lambda$ (lambda) that minimizes the cross-validated mean squared error in the *training* set. This value is stored as `cv$lambda.min`. To fit a lasso regression model, we set the parameter `alpha` to 1. The parameter `nfolds` sets the number of cross-validation folds.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(glmnet)\n",
                "set.seed (1)\n",
                "cv = cv.glmnet(x, y, alpha=1, lambda = grid, nfolds = 10)\n",
                "round(cv$lambda.min, 4)\n",
                "plot(cv)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The horizontal axis in this graph is drawn at logarithmic scale to show more detail. \"Log\" refers to the natural logarithm.\n",
                "The value of $\\lambda$ that minimizes the cross-validated mean squared error is shown at the left dotted line:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "round(log(cv$lambda.min),2)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The red dots are the point estimates of the prediction error and the gray bars are one standard error above and below the red dots. The right dotted line in the graph marks the \"second-best\" value of $\\lambda$ whose point estimate is one standard error larger than that of `cv$lambda.min`:\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "round(cv$lambda.1se,4)\n",
                "round(log(cv$lambda.1se),2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Compute the coefficient estimates for the lasso regression model that corresponds to `cv$lambda.min` and save the result as `lasso`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "lasso = glmnet(x, y, alpha=1, lambda=cv$lambda.min)\n",
                "round(coef(lasso),2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As shown in the top horizontal axis of the graph, the lasso model given by `cv$lambda.min` includes only 6 predictors. The predictor `meds` is dropped. The coefficients for `nurses` and `doctors` have shrunk from 0.51 and 0.11 to 0.47 and 0.10, respectively.\n",
                "\n",
                "If we wanted to shrink the coefficient estimates even further, we could use the \"second-best\" value of $\\lambda$:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "lasso.2 = glmnet(x, y, alpha=1, lambda=cv$lambda.1se)\n",
                "round(coef(lasso.2),2)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As shown in the graph, the more restrictive lasso model also drops `staff` as a predictor. The coefficients for `nurses` and `doctors` have shrunk from 0.47 and 0.10 to 0.42 and 0.05, respectively.\n",
                "\n",
                "Compute the predicted values of the response in the *test* set and the test error.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x = model.matrix(response ~ ., test)[,-1]\n",
                "predicted = predict(lasso, s=cv$lambda.min, newx=x)\n",
                "(mse_lasso = round(mean((predicted - test$response)^2),2))\n",
                "predicted.2 = predict(lasso.2, s=cv$lambda.min, newx=x)\n",
                "(mse_lasso.2 = round(mean((predicted.2 - test$response)^2),2))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### II. Single Pruned Tree\n",
                "\n",
                "Using the training set, grow the unpruned (\"fully grown\") tree.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(tree)\n",
                "tree = tree(response ~ ., data=train)\n",
                "summary(tree)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Plot the unpruned tree.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot(tree)\n",
                "text(tree, pretty = 0)\n",
                "title(main = \"Unpruned Classification Tree \\n\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Using the training set, compute the cross-validation error for subtrees of various sizes. The parameter `K` sets the number of cross-validation folds.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "set.seed(6295)\n",
                "cv = cv.tree(tree, K = 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Identify the tree size that minimizes the cross-validation error.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "(arg_min_cv = cv$size[which.min(cv$dev)])\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Plot the cross-validation error as a function of the tree size.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot(cv$dev ~ cv$size, type='b', col=\"lightseagreen\", lwd=2,\n",
                "     xlab = \"Subtree Size (Terminal Nodes)\", ylab = \"Cross-Validated Prediction Error\")\n",
                "axis(1, at=cv$size)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "In this example, the subtree size that minimizes the cross-validated mean squared error is identical to the size of the fully grown tree. Both the unpruned and the optimal tree have 9 terminal nodes. Thus, pruning will not affect the performance of this predictive model:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pruned_tree = prune.tree(tree, best = arg_min_cv)\n",
                "summary(pruned_tree)\n",
                "plot(pruned_tree)\n",
                "text(pruned_tree, pretty=0)\n",
                "title(main = \"Pruned Classification Tree \\n\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Compute the predicted values of the response in the *test* set and the test error.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "predicted = predict(pruned_tree, newdata=test)\n",
                "(mse_tree = round(mean((predicted - test$response)^2),2))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Plot the predicted against the actual ratings.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "par(pty = \"s\")\n",
                "plot(predicted ~ test$response, xlim=c(35,100), ylim = c(35,100), asp=1,\n",
                "     ylab = \"Predicted Rating\", xlab = \"Actual Rating\")\n",
                "abline(0,1)\n",
                "title(main = \"Single Pruned Tree\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Plot the predicted against the actual ratings.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "par(pty = \"m\")\n",
                "plot(predicted ~ test$response, xlim=c(35,100), ylim=c(68,70),\n",
                "     ylab = \"Predicted Rating\", xlab = \"Actual Rating\")\n",
                "abline(0,1)\n",
                "title(main = \"Single Pruned Tree: Detail\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### III. Bagging\n",
                "\n",
                "Using the training set, grow one unpruned tree for each of 500 bootstrap samples. The parameter `mtry` sets the number of predictors that are tried at each split.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(randomForest)\n",
                "set.seed(1)\n",
                "bag = randomForest(response ~ ., data = train, mtry = predictors, importance = TRUE)\n",
                "bag\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Compute the predicted values of the response in the *test* set and the test error.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "predicted = predict(bag, newdata=test)\n",
                "(mse_bag = round(mean((predicted - test$response)^2),2))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### IV. Random Forest\n",
                "\n",
                "Define $m = \\sqrt{p}$, the number of predictors tried at each split.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "(m = round(sqrt(predictors)))\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Grow a random forest of 500 trees. The parameter `mtry` sets the number of predictors that are tried at each split.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(randomForest)\n",
                "set.seed(1)\n",
                "rf = randomForest(response ~ ., data = train, mtry = m, importance = TRUE)\n",
                "rf\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Compute the predicted values of the response in the *test* set and the test error\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "predicted = predict(rf, newdata=test)\n",
                "(mse_rf = round(mean((predicted - test$response)^2),2))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Plot the predicted against the actual ratings.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "par(pty = \"s\")\n",
                "plot(predicted ~ test$response, xlim=c(35,100), ylim = c(35,100), asp=1,\n",
                "     ylab = \"Predicted Rating\", xlab = \"Actual Rating\")\n",
                "abline(0,1)\n",
                "title(main = \"Random Forest\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Plot the importance of each predictor\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "varImpPlot(rf)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### V. Boosting\n",
                "\n",
                "Grow a sequence of 5,000 trees.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "library(gbm)\n",
                "set.seed(1)\n",
                "boost = gbm(response ~ ., data = train, distribution = \"gaussian\", \n",
                "            n.trees=5000, interaction.depth=1)\n",
                "summary(boost, plotit = FALSE)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "Compute the predicted values of the response in the *test* set and the test error.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "predicted = predict(boost, newdata = test, n.trees=5000)\n",
                "(mse_boost = round(mean((predicted - test$response)^2),2))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### X. Summary\n",
                "\n",
                "**Prediction Method**   | **Test MSE**\n",
                "---                     | ---:\n",
                "Null Model              |\t55.19\n",
                "Linear Regression\t      | 14.40\n",
                "Ridge Regression        |\t14.39\n",
                "Lasso                   |\t14.38\n",
                "Single Pruned Tree      |\t18.92\n",
                "Bagging                 |\t13.60\n",
                "Random Forest           |\t13.31\n",
                "Boosting                |\t15.60\n",
                "\n",
                "\n",
                "Plot the test MSE values in a Cleveland dot plot.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x = c(mse_boost, mse_rf, mse_bag, mse_tree, mse_lasso, mse_ridge, mse_linear)\n",
                "l = c(\"Boosting\", \"Random Forest\", \"Bagging\", \"Single Pruned Tree\", \"Lasso\", \"Ridge Regression\", \"Linear Regression\")\n",
                "dotchart(x, labels = l, xlab = \"Test Mean Squared Error\",\n",
                "         color = ifelse(x==x[which.min(x)], \"red3\", \"black\"))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The graph shows how much the two ensemble methods, bagging and random forest, improve on the single pruned tree. The graph also shows how much the random-forest model, which only considers *a subset of* all available predictors at each split, improves on the bootstrap-aggregation (bagging) model, which considers all available predictors at each split.\n",
                "\n",
                "**Knowledge Check 1**\n",
                "\n",
                "Use the \"visits\" data set to predict `visits` as a function of all 11 predictors in that data set.\n",
                "\n",
                "Which prediction method achieves the lowest test error?\n",
                "\n",
                "How does the test error of the null model compare to the test errors of the tree -and regression-based methods that use the predictor variables?\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
